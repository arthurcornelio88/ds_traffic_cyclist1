import os
import json
import uuid
from typing import Optional
import datetime
from urllib.request import urlopen
import app.app_config as _  # forcer le sys.path side effect
from app.classes import RFPipeline, NNPipeline, AffluenceClassifierPipeline



def update_summary(
    summary_path: str,
    model_type: str,
    run_id: str,
    model_uri: str,
    env: str = "prod",
    test_mode: bool = False,
    rmse: float = None,
    r2: float = None,
    accuracy: float = None,
    precision: float = None,
    recall: float = None,
    f1_score: float = None
):

    entry = {
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "model_type": model_type,
        "env": env,
        "test_mode": test_mode,
        "run_id": run_id,
        "model_uri": model_uri,
    }

    # Ajoute les m√©triques si elles sont fournies
    if rmse is not None: entry["rmse"] = rmse
    if r2 is not None: entry["r2"] = r2
    if accuracy is not None: entry["accuracy"] = accuracy
    if precision is not None: entry["precision"] = precision
    if recall is not None: entry["recall"] = recall
    if f1_score is not None: entry["f1_score"] = f1_score

    # D√©terminer le chemin local
    summary_path_local = "/tmp/summary.json" if summary_path.startswith("gs://") else summary_path
    summary = []

    # T√©l√©charger ou charger si existant
    if summary_path.startswith("gs://"):
        os.system(f"gsutil cp {summary_path} {summary_path_local} || touch {summary_path_local}")
    if os.path.exists(summary_path_local):
        with open(summary_path_local, "r") as f:
            try:
                summary = json.load(f)
            except json.JSONDecodeError:
                print("‚ö†Ô∏è summary.json vide ou corrompu. R√©initialisation.")
                summary = []

    summary.append(entry)

    with open(summary_path_local, "w") as f:
        json.dump(summary, f, indent=2)

    if summary_path.startswith("gs://"):
        os.system(f"gsutil cp {summary_path_local} {summary_path}")
        print(f"‚úÖ summary.json mis √† jour et upload√© vers {summary_path}")
    else:
        print(f"‚úÖ summary.json mis √† jour localement : {summary_path}")

# === Chargement du meilleur mod√®le depuis le r√©sum√©
def get_best_model_from_summary(
    model_type: str,
    summary_path: str,
    env: str = "prod",
    metric: str = "rmse",
    test_mode: Optional[bool] = False # True pour test, False pour prod
):
    if summary_path.startswith("gs://"):
        summary = _read_gcs_json(summary_path)
    elif summary_path.startswith("http"):
        with urlopen(summary_path) as f:
            summary = json.load(f)
    else:
        with open(summary_path, "r") as f:
            summary = json.load(f)
    print(f"‚è≥ √âtape 1 ‚Äì Lecture du r√©sum√© depuis {summary_path}")
    print(f"‚è≥ √âtape 2 ‚Äì Filtrage sur model_type={model_type}, env={env}, test_mode={test_mode}")

    filtered = [
        r for r in summary
        if r["model_type"] == model_type
        and r["env"] == env
        and r["test_mode"] == test_mode
        #and r["rmse"] > 0  # √©viter les mod√®les fictifs/perfectibles
    ]

    if not filtered:
        raise RuntimeError(f"Aucun mod√®le trouv√© pour type={model_type}, env={env}, test_mode={test_mode}")

    metric_sorting = {
        "rmse": lambda r: -r["rmse"],
        "r2": lambda r: r["r2"],
        "f1_score": lambda r: r.get("f1_score", -1),
        "accuracy": lambda r: r.get("accuracy", -1)
    }

    if metric not in metric_sorting:
        raise ValueError(f"M√©trique non support√©e : {metric}")

    best = max(filtered, key=metric_sorting[metric])
    print(f"üîç R√©sum√© s√©lectionn√©:\n{json.dumps(best, indent=2)}")
    print(f"‚è≥ √âtape 3 ‚Äì T√©l√©chargement depuis GCS : {best['model_uri']}")

    value = best.get(metric, "N/A")
    print(f"‚úÖ Mod√®le {model_type} s√©lectionn√© : {best.get('run_id', 'N/A')} ({metric}={value})")

    local_model_path = _download_gcs_dir(best["model_uri"], prefix=model_type)
    print(f"‚è≥ √âtape 4 ‚Äì Chargement du mod√®le depuis {local_model_path}")

    # üîé Recherche automatique du sous-dossier portant le nom du mod√®le (ex: rf_class)
    subfolder = os.path.join(local_model_path, model_type)
    if os.path.isdir(subfolder):
        print(f"üìÅ Sous-dossier d√©tect√© pour {model_type}, on l'utilise : {subfolder}")
        local_model_path = subfolder
    else:
        print(f"‚ö†Ô∏è Aucun sous-dossier {model_type} trouv√© dans {local_model_path}")
        print(f"üìÇ Contenu d√©tect√© : {os.listdir(local_model_path)}")



    if model_type == "rf":
        return RFPipeline.load(local_model_path)
    elif model_type == "nn":
        return NNPipeline.load(local_model_path)
    elif model_type == "rf_class":
        return AffluenceClassifierPipeline.load(local_model_path)
    else:
        raise ValueError("Type de mod√®le non reconnu")


# === T√©l√©chargement GCS vers /tmp
def _download_gcs_dir(gcs_uri: str, prefix="model") -> str:
    from google.cloud import storage

    bucket_name, path = gcs_uri.replace("gs://", "").split("/", 1)
    local_tmp_dir = f"/tmp/{prefix}_{uuid.uuid4().hex}"
    os.makedirs(local_tmp_dir, exist_ok=True)

    client = storage.Client()
    blobs = list(client.list_blobs(bucket_name, prefix=path))

    print(f"üìÇ GCS path to download: {gcs_uri}")
    print(f"üì• Destination locale: {local_tmp_dir}")

    for blob in blobs:
        rel_path = os.path.relpath(blob.name, path)

        # ‚õî Ignore les "blobs de r√©pertoire"
        if rel_path in (".", "") or blob.name.endswith("/"):
            print(f"üö´ Blob ignor√© (r√©pertoire ou vide) : {blob.name}")
            continue

        local_file = os.path.join(local_tmp_dir, rel_path)
        os.makedirs(os.path.dirname(local_file), exist_ok=True)

        try:
            blob.download_to_filename(local_file)
            print(f"‚û°Ô∏è Fichier local : {local_file}")
        except Exception as e:
            print(f"üí• √âchec t√©l√©chargement : {blob.name}")
            print(f"‚ùå Exception : {e}")
            raise

    return local_tmp_dir


# === Lecture JSON depuis GCS
def _read_gcs_json(gs_path: str) -> dict:
    from google.cloud import storage

    if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
        raise EnvironmentError("Variable GOOGLE_APPLICATION_CREDENTIALS non d√©finie pour acc√©der √† GCS")

    bucket_name, blob_path = gs_path.replace("gs://", "").split("/", 1)
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_path)

    return json.loads(blob.download_as_text())
