import sys
import os

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

import streamlit as st
import pandas as pd
import os
import json
import time
import numpy as np
import app.app_config as _  # forcer le sys.path side effect
from app.model_registry_summary import get_best_model_from_summary

def get_secret(key, default=None):
    try:
        return st.secrets[key]
    except Exception:
        return os.getenv(key, default)

env = get_secret("env", "DEV")
gcp_raw = get_secret("gcp_service_account") or get_secret("GCP_SERVICE_ACCOUNT")

if gcp_raw and env == "PROD":
    gcp_dict = json.loads(gcp_raw) if isinstance(gcp_raw, str) else dict(gcp_raw)
    with open("/tmp/gcp.json", "w") as f:
        json.dump(gcp_dict, f)
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/gcp.json"
elif env == "DEV":
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "./gcp.json"

# === Fonction de chargement dynamique des mod√®les via summary.json ===
def fetch_best_pipeline(model_type: str):
    print("üì¶ Chargement de get_best_model_from_summary depuis model_registry_summary.py")
    return get_best_model_from_summary(
        model_type=model_type,
        summary_path="gs://df_traffic_cyclist1/models/summary.json",
        metric="r2",
        env="prod",
    )

@st.cache_resource
def load_best_pipeline(model_type: str):
    return fetch_best_pipeline(model_type)

# Chargement des pipelines
rf_pipeline = load_best_pipeline("rf")
# st.write("‚úÖ Random Forest charg√© :", type(rf_pipeline)) # DEBUG
st.write("‚úÖ Random Forest charg√© !")

nn_pipeline = load_best_pipeline("nn")
# st.write("‚úÖ Neural Net charg√© !", type(nn_pipeline)) # DEBUG
st.write("‚úÖ Neural Net charg√© !")

# === UI ===
st.sidebar.title("üß≠ Navigation")
page = st.sidebar.selectbox("Choisissez une page :", ["üîç Pr√©diction exemple", "üìÇ Pr√©diction CSV batch"])
st.title("üö≤ Pr√©diction du comptage horaire de v√©los")
modele = st.radio("Mod√®le √† utiliser :", ["Random Forest", "R√©seau de Neurones"])

# Fonction utilitaire pour obtenir le bon pipeline
def get_pipeline(name: str):
    return rf_pipeline if name == "Random Forest" else nn_pipeline

# === Exemples manuels
raw_samples = [
    {
        'nom_du_compteur': '35 boulevard de M√©nilmontant NO-SE',
        'date_et_heure_de_comptage': '2025-05-17 18:00:00+02:00',
        'coordonn√©es_g√©ographiques': '48.8672, 2.3501',
        'mois_annee_comptage': 'mai 2025'
    },
    {
        'nom_du_compteur': 'Totem 73 boulevard de S√©bastopol S-N',
        'date_et_heure_de_comptage': '2024-11-12 08:00:00+02:00',
        'coordonn√©es_g√©ographiques': '48.8639, 2.3895',
        'mois_annee_comptage': 'novembre 2024'
    },
    {
        'nom_du_compteur': "Quai d'Orsay E-O",
        'date_et_heure_de_comptage': '2024-06-03 15:00:00+02:00',
        'coordonn√©es_g√©ographiques': '48.8784, 2.3574',
        'mois_annee_comptage': 'juin 2024'
    }
]

# === Page 1 : pr√©diction manuelle
if page == "üîç Pr√©diction exemple":
    idx = st.selectbox("S√©lectionnez une observation brute √† pr√©dire :", range(len(raw_samples)), format_func=lambda i: f"Exemple {i+1}")
    sample_df = pd.DataFrame([raw_samples[idx]])
    pipeline = get_pipeline(modele)

    try:
        pred = pipeline.predict_clean(sample_df)[0]
        if isinstance(pred, (list, np.ndarray)) and isinstance(pred[0], (list, np.ndarray)):
            pred = pred[0]

        st.markdown("### üîç Observation s√©lectionn√©e")
        st.json(raw_samples[idx])
        st.success(f"üßæ Pr√©diction du comptage horaire : **{round(float(pred))} v√©los**")

    except Exception as e:
        st.error("Erreur lors de la pr√©diction :")
        st.code(str(e))
        st.exception(e)

# === Page 2 : pr√©diction sur CSV
elif page == "üìÇ Pr√©diction CSV batch":
    st.header("Pr√©diction sur fichier CSV brut")
    uploaded_file = st.file_uploader("Chargez un fichier brut (.csv)", type="csv")

    if uploaded_file is not None:
        df_csv = pd.read_csv(uploaded_file)
        pipeline = get_pipeline(modele)
        try:
            predictions = pipeline.predict(df_csv)
            predictions = predictions.flatten() if hasattr(predictions, "flatten") else predictions
            df_csv['prediction_comptage_horaire'] = predictions.round().astype(int)

            st.markdown("‚úÖ **R√©sultats :**")
            st.dataframe(df_csv.head(20))

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            file_name = f"predictions_{modele.lower().replace(' ', '_')}_{timestamp}.csv"
            csv_output = df_csv.to_csv(index=False).encode('utf-8')
            st.download_button("üì• T√©l√©charger les r√©sultats", csv_output, file_name=file_name, mime="text/csv")

        except Exception as e:
            st.error("Erreur lors de la pr√©diction :")
            st.code(str(e))
            st.exception(e)
