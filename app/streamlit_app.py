import sys
import os

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

import streamlit as st
import pandas as pd
import requests
import json
import time
import numpy as np
import app.app_config as _  # forcer le sys.path side effect
from app.model_registry_summary import get_best_model_from_summary
from PIL import Image
from streamlit_folium import st_folium
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import folium
from folium.plugins import MarkerCluster
from streamlit_folium import folium_static
import plotly.graph_objects as go


# === Configuration ===
API_URL = st.secrets["api_url"]
API_RF_CLASS_URL = st.secrets["api_rf_class_url"]
ENV = st.secrets["env"]
DATA_GS_URI = "gs://df_traffic_cyclist1/raw_data/comptage-velo-donnees-compteurs.csv"
DATA_LOCAL = "data/comptage-velo-donnees-compteurs.csv"

# === Authentification GCP ===
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = "-1"
try:
    gcp_secret = st.secrets["gcp_service_account"]
    with open("/tmp/gcp.json", "w") as f:
        json.dump(dict(gcp_secret), f)
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/tmp/gcp.json"
    print("‚úÖ GCP credentials configur√©s via Streamlit secrets")
except Exception as e:
    raise RuntimeError("‚ùå Credentials GCP manquants dans st.secrets.")

# === API Request Wrapper ===
def call_prediction_api(url: str, payload: dict, timeout: int = 60):
    try:
        with st.spinner("‚è≥ En attente de la r√©ponse du mod√®le..."):
            response = requests.post(url, json=payload, timeout=timeout)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.Timeout:
        st.error("‚è±Ô∏è L‚ÄôAPI a mis trop de temps √† r√©pondre. Elle est peut-√™tre en cold start.")
    except requests.exceptions.ConnectionError:
        st.error("üîå Impossible de se connecter √† l‚ÄôAPI. V√©rifier l‚ÄôURL ou la disponibilit√© du backend.")
    except requests.exceptions.HTTPError as http_err:
        st.error(f"‚ùå Erreur HTTP {response.status_code} : {response.text}")
    except requests.exceptions.RequestException as req_err:
        st.error(f"‚ö†Ô∏è Erreur inattendue : {type(req_err).__name__} ‚Äî {req_err}")
    return None

@st.cache_data
def load_clean_data(path):
    df = pd.read_csv(path, sep=';')
    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()
    df['date_et_heure_de_comptage'] = pd.to_datetime(df['date_et_heure_de_comptage'], utc=True).dt.tz_convert('Europe/Paris')
    df['heure'] = df['date_et_heure_de_comptage'].dt.hour
    df['jour_semaine'] = df['date_et_heure_de_comptage'].dt.day_name()
    df["mois"] = df["date_et_heure_de_comptage"].dt.month
    df[['latitude', 'longitude']] = df['coordonn√©es_g√©ographiques'].str.split(',', expand=True).astype(float)
    df = df.dropna(subset=['latitude', 'longitude'])
    return df

# === UI ===
st.sidebar.title("üß≠ Navigation")
menu = ["Pr√©sentation du projet", "Analyse Exploratoire", "D√©marche",
        "Mod√©lisation", "D√©mo"]

selection = st.sidebar.radio(label="", options=menu)

image = Image.open("docs/img/velo.jpg")
resized_image = image.resize((500, 200))
st.image(resized_image, use_container_width=True)

# Titre principal centr√©
st.markdown("""
<h1 style='text-align: center; font-size: 2em; margin-top: -10px;'>
     Pr√©diction du comptage horaire de v√©los
</h1>
<p style='text-align: center; font-size: 1.6em; color: grey; margin-top: -10px;'>
    De Novembre 2024 √† Novembre 2025
</p>
""", unsafe_allow_html=True)


if selection == "Pr√©sentation du projet":
    st.markdown("""
<div style="border:1px solid #ccc; padding:20px; border-radius:10px; text-align:center; font-size:16px">

<p>
Projet r√©alis√© dans le cadre de la formation <strong>Machine Learning Engineer</strong> de 
<a href="https://www.datascientest.com" target="_blank">DataScientest</a><br>
Promotion Alternance Novembre 2025
</p>

<p><strong>Auteurs :</strong><br>
<a href="https://www.linkedin.com/in/arthurcornelio/" target="_blank"><strong>Arthur CORNELIO</strong></a><br>
<a href="https://www.linkedin.com/in/brunohappi17/" target="_blank"><strong>Bruno HAPPI</strong></a><br>
<a href="https://www.linkedin.com/in/ibnemri/" target="_blank"><strong>Ibtihel NEMRI</strong></a>
</p>

<p><strong>Source de donn√©es :</strong><br>
<a href="https://opendata.paris.fr/explore/dataset/comptage-velo-donnees-compteurs" target="_blank">
Comptage v√©los | Open Data | Ville de Paris</a>
</p>

</div>
""", unsafe_allow_html=True)
    st.markdown("""
---

## üéØ Objectifs du projet

Ce projet a pour ambition de :

- **Analyser le trafic cycliste parisien** √† partir de donn√©es r√©colt√©es par des compteurs automatiques.
- Identifier les **zones et cr√©neaux horaires d‚Äôaffluence**.
- Construire des **mod√®les pr√©dictifs** pour anticiper le nombre de cyclistes en fonction du lieu, de la date et de l‚Äôheure.
- Fournir √† la Ville de Paris des **indicateurs utiles** pour optimiser ses politiques d‚Äôam√©nagements cyclables.

---

## üõ†Ô∏è M√©thodologie en bref

- Nettoyage et structuration des donn√©es : standardisation, extraction temporelle, traitement g√©ographique.
- Analyse statistique : identification de pics, moyennes par heure, jour, mois.
- Visualisations : cartes de chaleur, graphiques temporels, pics de trafic.
- Cr√©ation de **mod√®les de r√©gression** (Random Forest, R√©seau de Neurones) pour pr√©dire le **trafic horaire**.
- Construction d‚Äôun **mod√®le de classification binaire** pour d√©tecter les situations d‚Äô**affluence**.
- D√©ploiement dans une **interface interactive Streamlit** accessible et testable.

---

## üìä Exemples de visualisations

Quelques illustrations tir√©es de l‚Äôanalyse exploratoire :

""")

    st.image("docs/img/heatmap_static.png", caption="Carte des zones √† forte affluence cycliste", use_container_width=True)
    st.image("docs/img/traffic_hour_day.png", caption="Trafic moyen par heure selon le jour de la semaine", use_container_width=True)
    st.image("docs/img/pics_barplot.png", caption="Top 20 des pics de fr√©quentation par site de comptage", use_container_width=True)

    st.markdown("""
---

## üíª Application d√©ploy√©e

Les mod√®les les plus performants ont √©t√© int√©gr√©s √† une application Streamlit, permettant de :

- **Tester une pr√©diction** sur un exemple ou un fichier CSV.
- **Comparer plusieurs mod√®les** (Random Forest, R√©seau de Neurones et Random Forest Classifier).
- Obtenir des r√©sultats clairs, exploitables, et exportables.

‚û°Ô∏è Acc√©dez √† l‚Äôonglet **D√©mo** dans la barre lat√©rale pour utiliser l‚Äôinterface.

---
""")

elif selection == "Analyse Exploratoire":
    data_path = DATA_GS_URI if ENV == "PROD" else DATA_LOCAL
    df_clean = load_clean_data(data_path)
    st.header("üîç Analyse Exploratoire")
    st.markdown("""
    Dans cette section, nous explorons les donn√©es de comptage horaire des v√©los collect√©es par la Ville de Paris.  
    L‚Äôobjectif est de comprendre **les grandes tendances de mobilit√© cycliste**, de mettre en lumi√®re **les comportements selon l‚Äôheure, le jour ou le lieu**, et d‚Äôidentifier **les zones √† fort trafic**.

    Les analyses sont bas√©es sur un jeu de donn√©es brut que nous avons **nettoy√©, enrichi et structur√©**, pour le rendre exploitable √† des fins statistiques et de mod√©lisation.
    """)

    st.subheader("üì• Chargement et aper√ßu des donn√©es")
    st.markdown("""
    Le jeu de donn√©es est issu de la plateforme OpenData de la Ville de Paris.  
    Il recense les **comptages horaires de v√©los** effectu√©s par une centaine de capteurs r√©partis dans la ville.  
    Chaque ligne du dataset correspond √† **une heure de mesure pour un compteur donn√©**.

    Il contient les informations suivantes :
    - Date et heure de comptage
    - Nom et position du compteur
    - Nombre de v√©los compt√©s √† chaque heure
    - Coordonn√©es GPS du site de comptage
    """)
    st.write(df_clean.head())

    st.subheader("üßπ Nettoyage des donn√©es")
    st.markdown("""
    Avant toute analyse, plusieurs √©tapes de pr√©traitement ont √©t√© r√©alis√©es :

    - Normalisation des noms de colonnes pour faciliter la manipulation.
    - Conversion des dates au format `datetime` avec fuseau horaire de Paris.
    - Cr√©ation de variables temporelles utiles : **heure, jour, mois, jour de semaine**.
    - S√©paration des **coordonn√©es g√©ographiques** pour les visualisations cartographiques.
    - Suppression des colonnes superflues (liens vers photos, identifiants techniques, etc.).
    - Transformation des colonnes cat√©gorielles pour une meilleure performance m√©moire.
    """)

    st.markdown("### üìä Statistiques globales sur le comptage horaire")
    st.markdown("""
    Les premi√®res statistiques descriptives r√©v√®lent que :

    - Le trafic horaire moyen est de l‚Äôordre de quelques dizaines de v√©los.
    - Les valeurs maximales peuvent d√©passer plusieurs milliers de passages par heure sur certains axes.
    - Une grande dispersion est observ√©e, ce qui justifie une analyse segment√©e par heure et par jour.
    """)
    st.write(df_clean["comptage_horaire"].describe())

    st.markdown("### üìà Moyenne de v√©los par jour de la semaine")
    st.markdown("""
    Ce graphique permet d‚Äôobserver le comportement des cyclistes sur une semaine compl√®te.
    
    - Le trafic est plus √©lev√© en semaine, en particulier du mardi au jeudi.
    - Une baisse notable est visible le samedi et surtout le dimanche.
    - Cela refl√®te clairement l‚Äôusage utilitaire du v√©lo pour les trajets domicile-travail.
    """)
    st.bar_chart(df_clean.groupby("jour_semaine")["comptage_horaire"].mean())

    st.markdown("### üïí Moyenne par heure de la journ√©e")
    st.markdown("""
    On observe deux pics majeurs :

    - Entre 7h30 et 9h le matin.
    - Entre 17h30 et 19h le soir.

    Ces pics correspondent parfaitement aux horaires de travail traditionnels, ce qui confirme l‚Äôusage pendulaire du v√©lo √† Paris.
    """)
    st.line_chart(df_clean.groupby("heure")["comptage_horaire"].mean())

    st.markdown("### üìÜ Moyenne de trafic cycliste par mois")
    st.markdown("""
    Cette analyse montre une saisonnalit√© tr√®s marqu√©e :

    - Les mois les plus actifs sont mai, juin, juillet et septembre.
    - Le creux hivernal (d√©cembre √† f√©vrier) est bien visible.

    Ces variations sont corr√©l√©es √† la m√©t√©o et √† la dur√©e d‚Äôensoleillement, deux facteurs qui influencent fortement la pratique du v√©lo.
    """)
    mois_mean = df_clean.groupby("mois")["comptage_horaire"].mean()
    st.bar_chart(mois_mean)

    st.markdown("### üìä Comparaison du trafic : semaine vs week-end")
    st.markdown("""
    En comparant les courbes :

    - En semaine, les pics sont nets et concentr√©s sur les horaires de bureau.
    - Le week-end, le trafic est plus r√©gulier et r√©parti tout au long de la journ√©e.

    Cela traduit un usage loisir ou touristique le samedi et le dimanche, contre un usage fonctionnel en semaine.
    """)
    df_clean['type_jour'] = df_clean['jour_semaine'].apply(lambda x: 'weekend' if x in ['Saturday', 'Sunday'] else 'semaine')
    trafic_jour_type = df_clean.groupby(['type_jour', 'heure'])['comptage_horaire'].mean().reset_index()
    plt.figure(figsize=(10, 5))
    sns.lineplot(data=trafic_jour_type, x='heure', y='comptage_horaire', hue='type_jour', palette='Set1')
    plt.title("Comparaison du trafic : semaine vs week-end")
    plt.xlabel("Heure")
    plt.ylabel("V√©los/heure")
    st.pyplot(plt.gcf())

    st.markdown("### üìç Top 10 des compteurs les plus fr√©quent√©s")
    st.markdown("""
    Les compteurs les plus actifs sont tous situ√©s sur des axes majeurs :

    - Boulevard de S√©bastopol
    - Boulevard de M√©nilmontant
    - Quai d'Orsay

    Ces zones concentrent une grande partie du trafic cycliste parisien et sont souvent bien am√©nag√©es pour les d√©placements √† v√©lo.
    """)
    top_compteurs = df_clean.groupby("nom_du_compteur")["comptage_horaire"].sum().nlargest(10)
    st.bar_chart(top_compteurs)

    st.markdown("### üîù Top 20 des pics horaires de trafic cycliste")
    st.markdown("""
    Cette analyse isole les moments o√π le trafic a atteint un record horaire pour chaque compteur :

    - Ces pics se produisent quasi exclusivement en semaine.
    - Ils surviennent autour de 8h30 ou 18h, aux heures de pointe.

    Cela confirme la n√©cessit√© de renforcer l‚Äôinfrastructure cyclable sur ces cr√©neaux horaires.
    """)
    df_time_series = df_clean.groupby(['nom_du_compteur', 'date_et_heure_de_comptage'])['comptage_horaire'].mean().unstack(level=0)
    pics_absolus = df_time_series.idxmax()
    analyse_pics = pd.DataFrame({
        'Compteur': pics_absolus.index,
        'Heure_du_pic': pics_absolus.values,
        'Trafic_max': df_time_series.max().values
    }).sort_values('Trafic_max', ascending=False)
    analyse_pics['Heure_du_pic'] = analyse_pics['Heure_du_pic'].dt.strftime('%Y-%m-%d %H:%M')
    fig = px.bar(analyse_pics.head(20), x='Compteur', y='Trafic_max', hover_data=['Heure_du_pic'],
                 color='Trafic_max', title='<b>Top 20 des pics de trafic cycliste</b>',
                 labels={'Trafic_max': 'V√©los/heure', 'Heure_du_pic': 'Moment du pic'},
                 height=500)
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig)

    st.markdown("### üìÖ Trafic moyen par heure selon le jour de la semaine")
    st.markdown("Le trafic varie sensiblement entre semaine et week-end. Les lundis et vendredis ont des profils distincts.")
    df_jour = df_clean.groupby(['jour_semaine', 'heure'])['comptage_horaire'].mean().reset_index()
    ordre_jours = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    plt.figure(figsize=(14, 8))
    sns.lineplot(data=df_jour, x='heure', y='comptage_horaire', hue='jour_semaine',
                 hue_order=ordre_jours, palette='viridis', linewidth=2.5)
    plt.title('Trafic cycliste moyen par heure et jour de semaine', fontsize=16)
    plt.xlabel('Heure de la journ√©e')
    plt.ylabel('Nombre moyen de v√©los/heure')
    plt.grid(alpha=0.3)
    plt.legend(title='Jour', bbox_to_anchor=(1.05, 1))
    st.pyplot(plt.gcf())

    st.markdown("""
    ### üßæ Conclusion de l‚Äôanalyse exploratoire
    - Le trafic cycliste suit un cycle horaire et hebdomadaire r√©gulier, avec des pics matin et soir en semaine.
    - Les week-ends montrent un trafic plus homog√®ne dans la journ√©e.
    - Les zones centrales comme les boulevards ou les quais sont les plus fr√©quent√©es.
    - Les donn√©es sont de qualit√© et pr√™tes pour la mod√©lisation pr√©dictive.
    """)

elif selection == "D√©marche":
    st.header("üß≠ D√©marche")
    st.markdown("""
    Apr√®s avoir explor√© les donn√©es de comptage horaire des v√©los √† Paris, l‚Äôobjectif est d√©sormais de **pr√©voir le trafic cycliste** √† partir de variables temporelles et g√©ographiques.
    
    ### üéØ Objectifs de mod√©lisation

    Deux axes principaux ont guid√© notre d√©marche :

    1. **Pr√©voir le nombre de v√©los par heure** sur un compteur donn√© (mod√®le de r√©gression).
    2. **D√©tecter les situations d‚Äôaffluence** √† partir d‚Äôun seuil propre √† chaque compteur (mod√®le de classification binaire).

    ---

    ### üß© S√©lection des variables explicatives

    Pour estimer le trafic cycliste, nous avons retenu les variables suivantes :

    - **Heure de la journ√©e** : le trafic est tr√®s d√©pendant des horaires (pics de pointe).
    - **Jour de la semaine** : diff√©rence marqu√©e entre semaine et week-end.
    - **Mois de l‚Äôann√©e** : influence saisonni√®re significative.
    - **Coordonn√©es GPS** du compteur : certaines zones sont structurellement plus fr√©quent√©es.

    Ces variables permettent de capter les **effets temporels et spatiaux** du trafic v√©lo.

    ---

    ### üîÑ Pr√©traitement des donn√©es

    Avant l'entra√Ænement des mod√®les, plusieurs transformations ont √©t√© appliqu√©es :

    - Encodage des variables cat√©gorielles (`jour_semaine`, `nom_du_compteur`) via :
      - `OneHotEncoder` pour les mod√®les de r√©gression.
      - `LabelEncoder` pour les mod√®les de classification.
    - S√©paration du jeu de donn√©es en un **jeu d‚Äôentra√Ænement (80‚ÄØ%)** et un **jeu de test (20‚ÄØ%)**.
    - Conversion des coordonn√©es g√©ographiques en `float32` pour optimiser la m√©moire.

    ---

    ### üîç Approche r√©gressive

    Pour pr√©dire le **nombre de v√©los par heure**, nous avons test√© plusieurs mod√®les :

    - **R√©gression lin√©aire** : comme mod√®le de base pour √©valuer les performances minimales attendues.
    - **Random Forest Regressor** : capable de g√©rer les non-lin√©arit√©s et les interactions complexes.
    - **XGBoost Regressor** : algorithme puissant souvent performant sur des donn√©es structur√©es.

    Les pr√©dictions sont √©valu√©es √† l‚Äôaide de **m√©triques classiques** : MAE (erreur absolue moyenne) et R¬≤ (variance expliqu√©e).

    ---

    ### ‚ö†Ô∏è D√©tection d'affluence

    En parall√®le, nous avons formul√© un **probl√®me de classification binaire** :

    - Une ligne est marqu√©e comme une **situation d‚Äôaffluence** (`Affluence = 1`) si le comptage horaire d√©passe la moyenne historique du compteur concern√©.
    - Sinon, elle est marqu√©e comme `Affluence = 0`.

    Cette cible binaire permet d‚Äôentra√Æner un **RandomForestClassifier** capable d‚Äôanticiper les p√©riodes o√π l‚Äôinfrastructure cyclable est susceptible d‚Äô√™tre satur√©e.

    L‚Äô√©valuation du mod√®le se base sur des indicateurs classiques : **accuracy, pr√©cision, rappel, F1-score**, ainsi que **la courbe ROC et l‚ÄôAUC**.

    ---

    ### üîí Fiabilit√© et d√©ploiement

    Pour garantir la reproductibilit√© et une future int√©gration :

    - Les meilleurs mod√®les sont **s√©rialis√©s (via joblib)**.
    - Les **r√©sum√©s de performances sont stock√©s en JSON**, pour suivi ou visualisation ult√©rieure.
    - La pr√©diction ponctuelle est possible √† partir de **valeurs connues** (ex. : heure, jour, compteur‚Ä¶).
    """)

elif selection == "Mod√©lisation":
    st.header("üß† Mod√©lisation")
    st.write("Nous avons utilis√© un RandomForestClassifier pour d√©tecter l‚Äôaffluence...")
    modele = st.radio("Mod√®le √† utiliser :", ["Random Forest", "R√©seau de Neurones", "Random Forest Classifier"])
    if modele == "Random Forest":
        
        # Configuration de la page
        st.title("üö¥‚Äç‚ôÇÔ∏è Mod√©lisation Pr√©dictive du Trafic Cycliste")

        # Section 1: Pr√©sentation des donn√©es
        with st.expander("üìä Donn√©es Utilis√©es", expanded=True):
            st.markdown("""
            **Variables utilis√©es dans le mod√®le :**
            - Variable cible : `comptage_velo` (continu)
            - Features :
                - `nom_du_compteur` (label encod√©)
                - Variables cycliques : `heure_sin`, `heure_cos`, `mois_sin`, `mois_cos`
                - `jour_semaine` (one-hot encoded)
                - `annee` (binaire)
            """)

        # Section 2: R√©sultats de validation crois√©e
        st.header("üìà R√©sultats de Validation Crois√©e")

        cv_scores = [0.91515524, 0.91597844, 0.91690901]
        mean_cv_score = np.mean(cv_scores)

        col1, col2 = st.columns(2)
        with col1:
            st.metric("Moyenne R¬≤ (CV)", f"{mean_cv_score:.4f}", delta_color="off")
        with col2:
            st.metric("√âcart-type (CV)", f"{np.std(cv_scores):.6f}", delta_color="off")

        # Section 3: R√©sultats sur le test set
        st.header("üß™ Performance sur le Test Set")

        # M√©triques du test set
        rmse_rf = 30.13
        r2_rf = 0.9163

        col1, col2 = st.columns(2)
        with col1:
            st.metric("RMSE (Test Set)", f"{rmse_rf:.2f}", delta_color="off")
        with col2:
            st.metric("R¬≤ (Test Set)", f"{r2_rf:.4f}", delta_color="off")

        # Visualisation comparative
        st.subheader("Comparaison Validation Crois√©e / Test Set")
        fig = go.Figure()

        fig.add_trace(go.Bar(
            x=['Validation Crois√©e', 'Test Set'],
            y=[mean_cv_score, r2_rf],
            name='R¬≤ Score',
            text=[f"{mean_cv_score:.4f}", f"{r2_rf:.4f}"],
            textposition='auto',
            marker_color=['#3498db', '#2ecc71']
        ))

        fig.update_layout(
            yaxis_title='Score R¬≤',
            title='Comparaison des Performances',
            showlegend=False
        )
        st.plotly_chart(fig, use_container_width=True)

        # Section 4: Interpr√©tation
        with st.expander("üîç Analyse des R√©sultats", expanded=True):
            st.markdown(f"""
            **Interpr√©tation des performances :**
            
            - üéØ **Score R¬≤ de {r2_rf:.4f} sur le test set** - Le mod√®le explique 91.63% de la variance des donn√©es de test
            - üìè **RMSE de {rmse_rf:.2f}** - L'erreur moyenne est d'environ 30 v√©los/heure
            - üîÑ **Coh√©rence entre CV et test set** - Pas de surapprentissage d√©tect√© (R¬≤ similaire)
            
            **Points forts :**
            - Robustesse confirm√©e sur donn√©es non vues
            - Bonne g√©n√©ralisation gr√¢ce √† la validation crois√©e rigoureuse
            - Faible √©cart entre performance d'entra√Ænement et de test
            
            **Recommandations :**
            - Tester avec plus de donn√©es historiques
            - Ajouter des variables contextuelles (m√©t√©o, √©v√©nements)
            - Optimiser les hyperparam√®tres avec GridSearch
            """)

       


    elif modele == "R√©seau de Neurones":

    # Section 1: Architecture du mod√®le
        with st.expander("üèó Architecture du Mod√®le", expanded=True):
            st.markdown("""
        **Structure innovante combinant :**
        - Couche d'Embedding pour les compteurs v√©lo
        - Features cycliques standardis√©es
        - Architecture profonde avec r√©gularisation
        """)
        
        
            st.code("""
    # Couches principales
    Embedding(input_dim=n_compteurs, 
            output_dim=8)

    Dense(128, activation='relu')
    Dropout(0.3)

    Dense(64, activation='relu')
    Dropout(0.2)

    Dense(32, activation='relu')
    Dense(1)  # Sortie
            """, language='python')
        
        # Section 3: Performance
        st.header("üìä R√©sultats sur le Test Set")

        nn_metrics = {
            "RMSE": 36.24,
            "R¬≤": 0.8789,
            "MAE": 19.12  # Exemple
        }

        col1, col2, col3 = st.columns(3)
        col1.metric("RMSE", f"{nn_metrics['RMSE']:.2f}")
        col2.metric("R¬≤ Score", f"{nn_metrics['R¬≤']:.4f}")
        col3.metric("MAE", f"{nn_metrics['MAE']:.2f}")

        # Section 4: Analyse technique
        with st.expander("üß† Points Cl√©s du Mod√®le", expanded=True):
            st.markdown("""
            **Innovations majeures :**
            - üß© **Embedding des compteurs** : Capture les similarit√©s entre sites de comptage
            - üïí **Features cycliques** : Heure/mois en sin/cos pour pr√©server la cyclicit√©
            - üõ° **R√©gularisation** : Dropout pour √©viter le surapprentissage
            
            **Avantages :**
            - Meilleure capture des motifs complexes
            - Flexibilit√© architecturale
            - Performance proche du Random Forest (R¬≤ = 0.879)
            
            **Am√©liorations possibles :**
            - Ajouter des features m√©t√©o
            - Tester des architectures attention
            - Optimiser les hyperparam√®tres
            """)


    else :
        st.write("Random Forest Classifier est un mod√®le d'ensemble qui combine plusieurs arbres de d√©cision. Il est robuste et peut g√©rer des donn√©es manquantes et non lin√©aires. Il est adapt√© aux donn√©es cat√©gorielles et num√©riques.")
        st.header("üîç Documentation ‚Äì RandomForestClassifier")
        st.markdown("""
    **1. Contexte**  
    Nous utilisons ce mod√®le pour une **classification binaire** : pr√©dire si une p√©riode horaire correspond √† une **affluence** (`1`) ou non (`0`).

    ---

    **2. Mod√®le RandomForestClassifier**  
    Il s‚Äôagit d‚Äôun algorithme d‚Äôensemble combinant plusieurs arbres de d√©cision, chacun entra√Æn√© sur un sous-√©chantillon al√©atoire des donn√©es. La pr√©diction finale est obtenue par vote majoritaire, ce qui am√©liore la robustesse et att√©nue le surapprentissage :contentReference[oaicite:3]{index=3}.

    ---

    **3. Pourquoi ce choix ?**  
    - R√©duit le **surapprentissage** gr√¢ce √† l'agr√©gation des arbres.  
    - Manipule directement les variables num√©riques et cat√©gorielles, sans normalisation pr√©alable :contentReference[oaicite:4]{index=4}.  
    - Robuste au **bruit** et aux **valeurs manquantes**.  
    - Donne des mesures d‚Äô**importance des variables** (impuret√©, permutation) :contentReference[oaicite:5]{index=5}.

    ---

    **4. Hyperparam√®tres utilis√©s**  
    - `n_estimators = 100` : nombre d‚Äôarbres.  
    - `random_state = 42` : assure la reproductibilit√©.

    ---

    **5. √âvaluation sur le jeu test**  
    | M√©trique   | Valeur typique | Interpr√©tation |
    |------------|----------------|----------------|
    | Accuracy   | ~86‚ÄØ%          | Pourcentage global de bonnes pr√©dictions |
    | Precision  | ~82‚ÄØ%          | Parmi les heures pr√©dites comme affluence (`1`), 82‚ÄØ% √©taient correctes.:contentReference[oaicite:6]{index=6} |
    | Recall     | ~82‚ÄØ%          | Parmi les vraies heures d‚Äôaffluence, 82‚ÄØ% ont √©t√© d√©tect√©es. :contentReference[oaicite:7]{index=7} |
    | F1-score   | ~0.82          | Harmonie entre precision et recall, adapt√© aux classes d√©s√©quilibr√©es. :contentReference[oaicite:8]{index=8} |

    **Interpr√©tation**  
    - **Precision** √©lev√©e ‚Üí peu de fausses alertes.  
    - **Recall** √©lev√© ‚Üí peu de pics rat√©s.  
    - **F1-score** √©lev√© (~0,82) ‚Üí bon compromis entre sensibilit√© et fiabilit√©.
    - **Accuracy** est utile, mais insuffisante si la classe "Affluence" est minoritaire :contentReference[oaicite:9]{index=9}.

    ---

    **6. Repr√©sentation visuelle recommand√©e**  
    Pour analyser davantage, on peut afficher :
    - La **matrice de confusion**, pour visualiser les vrais/faux positifs et n√©gatifs.""")
        st.image("docs/img/matrice_de_confusion.png", caption="Matrice de confusion", use_container_width=True)
        st.markdown("""
    - La **courbe ROC-AUC** pour voir le compromis entre taux de vrais positifs et taux de faux positifs.""")
        st.image("docs/img/Courbe_ROC.png", caption="courbe ROC-AUC", use_container_width=True)
        st.markdown("""
    ---

    **Conclusion :**  
    Ce mod√®le offre un bon compromis pour d√©tecter les p√©riodes d'affluence :  
    - **Fiabilit√©** (peu de fausses alertes) gr√¢ce √† la precision √©lev√©e.  
    - **Sensibilit√©** (peu de pics manqu√©s) gr√¢ce au recall √©lev√©.  
    - **√âquilibre** entre les deux gr√¢ce √† un F1-score sup√©rieur √† 0,8, ce qui est id√©al en contexte binaire avec classe minoritaire.
    """)
    # Tu peux √©galement int√©grer ta documentation du mod√®le ici
elif selection == "D√©mo":
    st.header("üö≤ D√©mo de pr√©diction du trafic")
    st.write("Interface de d√©monstration avec exemples manuels ou fichier CSV...")

    page = st.sidebar.selectbox("Choisissez une page :", ["üîç Pr√©diction exemple", "üìÇ Pr√©diction CSV batch"])

    model_map = {
    "Random Forest": ("rf", "r2"),
    "R√©seau de Neurones": ("nn", "r2"),
    "RF Classifier (Affluence)": ("rf_class", "f1_score")
    }
    model_choice = st.radio("Mod√®le √† utiliser :", list(model_map.keys()))
    model_type, metric = model_map[model_choice]

    # === Exemples manuels
    raw_samples = [
        {
            'nom_du_compteur': '35 boulevard de M√©nilmontant NO-SE',
            'date_et_heure_de_comptage': '2025-05-17 18:00:00+02:00',
            'coordonn√©es_g√©ographiques': '48.8672, 2.3501',
            'mois_annee_comptage': 'mai 2025'
        },
        {
            'nom_du_compteur': 'Totem 73 boulevard de S√©bastopol S-N',
            'date_et_heure_de_comptage': '2024-11-12 08:00:00+02:00',
            'coordonn√©es_g√©ographiques': '48.8639, 2.3895',
            'mois_annee_comptage': 'novembre 2024'
        },
        {
            'nom_du_compteur': "Quai d'Orsay E-O",
            'date_et_heure_de_comptage': '2024-06-03 15:00:00+02:00',
            'coordonn√©es_g√©ographiques': '48.8784, 2.3574',
            'mois_annee_comptage': 'juin 2024'
        }
    ]

    # === Page 1 : pr√©diction manuelle
    if page == "üîç Pr√©diction exemple":
        idx = st.selectbox("S√©lectionnez une observation :", range(len(raw_samples)), format_func=lambda i: f"Exemple {i+1}")
        selected = raw_samples[idx]
        st.markdown("### üîç Observation s√©lectionn√©e")
        st.json(selected)

        if st.button("üîÆ Lancer la pr√©diction"):
            payload = {
                "records": [selected],
                "model_type": model_type,
                "metric": metric
            }
            #api_url = API_URL  # No RF Class URL for this example, testing
            api_url = API_RF_CLASS_URL if model_type == "rf_class" else API_URL
            #st.write("üîß Payload envoy√© :", payload)
            #st.write("üîó API URL :", api_url)
            result = call_prediction_api(api_url, payload)
            if result:
                pred = result["predictions"][0]
                if model_type == "rf_class":
                    st.success("üìä Affluence d√©tect√©e ‚úÖ" if pred == 1 else "üìâ Faible fr√©quentation attendue")
                else:
                    # g√®re les deux cas : [val] ou [[val]]
                    pred = pred[0] if isinstance(pred, (list, tuple)) else pred
                    st.success(f"üßæ Pr√©diction du comptage horaire : **{round(float(pred))} v√©los**")
    
        with st.expander("ü©∫ Debug API"):
            if st.button("üîÅ Forcer ping API"):
                try:
                    ping_response = requests.get(API_URL.replace("/predict", "/docs"), timeout=10)
                    if ping_response.status_code == 200:
                        st.success("‚úÖ API en ligne (endpoint /docs accessible).")
                    else:
                        st.warning(f"‚ö†Ô∏è API r√©pond mais code inattendu : {ping_response.status_code}")
                except Exception as e:
                    st.error(f"‚ùå API inaccessible : {e}")
            if st.button("üîÑ Forcer /refresh_models"):
                try:
                    refresh_url = API_URL.replace("/predict", "/refresh_models")
                    refresh_response = requests.post(refresh_url, timeout=15)
                    if refresh_response.status_code == 200:
                        st.success("‚úÖ Mod√®les recharg√©s depuis /refresh_models.")
                        st.json(refresh_response.json())
                    else:
                        st.warning(f"‚ö†Ô∏è Requ√™te envoy√©e mais r√©ponse inattendue : {refresh_response.status_code}")
                except Exception as e:
                    st.error(f"‚ùå √âchec du refresh : {e}")


    # === Page CSV ===
    elif page == "üìÇ Pr√©diction CSV batch":
        st.header("Pr√©diction sur fichier CSV brut")
        uploaded_file = st.file_uploader("Chargez un fichier brut (.csv)", type="csv")

        if uploaded_file is not None:
            df_csv = pd.read_csv(uploaded_file)
            payload = {
                "records": df_csv.to_dict(orient="records"),
                "model_type": model_type,
                "metric": metric
            }
            #api_url = API_URL  # No RF Class URL for this batch processing
            api_url = API_RF_CLASS_URL if model_type == "rf_class" else API_URL
            #st.write("üîß Payload envoy√© :", payload)
            #st.write("üîó API URL :", api_url)
            result = call_prediction_api(api_url, payload)
            if result:
                predictions = result["predictions"]
                predictions = np.array(predictions).flatten()
                df_csv["prediction_comptage_horaire"] = predictions.round().astype(int) if model_type != "rf_class" else predictions.astype(int)

                st.markdown("‚úÖ **R√©sultats :**")
                st.dataframe(df_csv.head(20))

                timestamp = time.strftime("%Y%m%d_%H%M%S")
                file_name = f"predictions_{model_type}_{timestamp}.csv"
                csv_output = df_csv.to_csv(index=False).encode("utf-8")
                st.download_button("üì• T√©l√©charger les r√©sultats", csv_output, file_name=file_name, mime="text/csv")






